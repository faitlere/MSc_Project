# -*- coding: utf-8 -*-
"""Predictive Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-jo5qm3zokmmQrM_iCe5Qg7IW48KaM26
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, classification_report
import sklearn as sk

# Load data
df = pd.read_csv("/content/HR_Analytics.csv")

# Create target and binary variables
def years_to_stage(years):
    if years < 1:   return 3  # Recently promoted
    if years < 2:   return 2  # On track
    if years < 5:   return 1  # Slow
    return 0                # Stalled

df['Career_Stage_Ordinal'] = df['YearsSinceLastPromotion'].apply(years_to_stage)
y_ord = df['Career_Stage_Ordinal']
y_bin = (y_ord >= 2).astype(int)  # advancing vs not

# Select features
features = [
    'YearsAtCompany','YearsInCurrentRole','JobLevel','MonthlyIncome','Age',
    'PerformanceRating','TrainingTimesLastYear',
    'JobRole','Department','Gender','OverTime','BusinessTravel','MaritalStatus',
    'Attrition'
]
X = df[features].copy()

# Train/test split
X_train, X_test, y_train_ord, y_test_ord = train_test_split(
    X, y_ord, test_size=0.2, random_state=42, stratify=y_ord
)
# binary targets aligned with same split
y_train_bin = (y_train_ord >= 2).astype(int)
y_test_bin  = (y_test_ord  >= 2).astype(int)

# Preprocessing: scale numerics, one-hot categoricals
num_cols = ['YearsAtCompany','YearsInCurrentRole','JobLevel','MonthlyIncome','Age',
            'PerformanceRating','TrainingTimesLastYear']
cat_cols = ['JobRole','Department','Gender','OverTime','BusinessTravel','MaritalStatus','Attrition']

preprocess = ColumnTransformer([
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
])

# Models
log_reg = Pipeline([
    ("prep", preprocess),
    ("clf", LogisticRegression(max_iter=500, class_weight="balanced"))
])

rf_clf = Pipeline([
    ("prep", preprocess),
    ("clf", RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        n_jobs=-1
    ))
])

#  Train
log_reg.fit(X_train, y_train_bin)
rf_clf.fit(X_train, y_train_ord)

# Evaluate
def eval_model(name, y_true, y_pred):
    print(f"\n{name}")
    print("  Accuracy:         ", accuracy_score(y_true, y_pred))
    print("  Balanced accuracy:", balanced_accuracy_score(y_true, y_pred))
    print("  Macro F1:         ", f1_score(y_true, y_pred, average='macro'))

# Logistic
y_pred_bin = log_reg.predict(X_test)
eval_model("Logistic ", y_test_bin, y_pred_bin)
print("\nClassification report:")
print(classification_report(y_test_bin, y_pred_bin, target_names=["Not advancing","Advancing"]))

# Random Forest
y_pred_ord = rf_clf.predict(X_test)
eval_model("Random Forest", y_test_ord, y_pred_ord)
print("\nClassification report :")
print(classification_report(
    y_test_ord, y_pred_ord,
    target_names=["Stalled(0)","Slow(1)","OnTrack(2)","Recent(3)"]
))

## Check for fairness
male = X_test['Gender'] == 'Male'
female = X_test['Gender'] == 'Female'

male_accuracy = accuracy_score(y_test_bin[male], y_pred_bin[male])
female_accuracy = accuracy_score(y_test_bin[female], y_pred_bin[female])

print(f"Male accuracy: {male_accuracy:.3f}")
print(f"Female accuracy: {female_accuracy:.3f}")
print(f"Fairness gap: {abs(male_accuracy - female_accuracy):.3f}")

# Feature importances
ohe = rf_clf.named_steps["prep"].named_transformers_["cat"]
ohe_names = list(ohe.get_feature_names_out(cat_cols))
all_features = num_cols + ohe_names
importances = rf_clf.named_steps["clf"].feature_importances_
fi = pd.DataFrame({"feature": all_features, "importance": importances})
fi = fi.sort_values("importance", ascending=False).head(15)
print("\nTop 15 RF feature importances:")
print(fi.to_string(index=False))

cv_bal_acc = cross_val_score(log_reg, X_train, y_train_bin, cv=5, scoring='balanced_accuracy')
print(f"\nLogistic CV balanced accuracy: {cv_bal_acc.mean():.4f} (+/- {cv_bal_acc.std():.4f})")